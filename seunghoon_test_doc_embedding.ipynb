{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "358c3903-acee-442e-850b-197e1774b3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from glob import glob\n",
    "from itertools import islice\n",
    "import tiktoken\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcbca40e-5287-44be-bbb1-8e519a50d2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETIONS_MODEL = \"text-davinci-003\"\n",
    "EMBEDDING_MODEL = 'text-embedding-ada-002'\n",
    "EMBEDDING_CTX_LENGTH = 8191\n",
    "EMBEDDING_ENCODING = 'cl100k_base'\n",
    "\n",
    "COMPLETIONS_API_PARAMS = {\n",
    "    # We use temperature of 0.0 because it gives the most predictable, factual answer.\n",
    "    \"temperature\": 0.0,\n",
    "    \"max_tokens\": 500,\n",
    "    \"model\": COMPLETIONS_MODEL,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6796561-d65f-43a5-b053-ac3f38bd6da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str, model: str=EMBEDDING_MODEL) -> list[float]:\n",
    "    result = openai.Embedding.create(\n",
    "      model=model,\n",
    "      input=text\n",
    "    )\n",
    "    return result[\"data\"][0][\"embedding\"]\n",
    "\n",
    "def truncate_text_tokens(text, encoding_name=EMBEDDING_ENCODING, max_tokens=EMBEDDING_CTX_LENGTH):\n",
    "    \"\"\"Truncate a string to have `max_tokens` according to the given encoding.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    return encoding.encode(text)[:max_tokens]\n",
    "\n",
    "def batched(iterable, n):\n",
    "    \"\"\"Batch data into tuples of length n. The last batch may be shorter.\"\"\"\n",
    "    # batched('ABCDEFG', 3) --> ABC DEF G\n",
    "    if n < 1:\n",
    "        raise ValueError('n must be at least one')\n",
    "    it = iter(iterable)\n",
    "    while (batch := tuple(islice(it, n))):\n",
    "        yield batch\n",
    "        \n",
    "def chunked_tokens(text, encoding_name, chunk_length):\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    tokens = encoding.encode(text)\n",
    "    chunks_iterator = batched(tokens, chunk_length)\n",
    "    yield from chunks_iterator\n",
    "    \n",
    "def len_safe_get_embedding(text, model=EMBEDDING_MODEL, max_tokens=EMBEDDING_CTX_LENGTH, encoding_name=EMBEDDING_ENCODING, average=True):\n",
    "    chunk_embeddings = []\n",
    "    chunk_lens = []\n",
    "    for chunk in chunked_tokens(text, encoding_name=encoding_name, chunk_length=max_tokens):\n",
    "        chunk_embeddings.append(get_embedding(chunk, model=model))\n",
    "        chunk_lens.append(len(chunk))\n",
    "\n",
    "    if average:\n",
    "        chunk_embeddings = np.average(chunk_embeddings, axis=0, weights=chunk_lens)\n",
    "        chunk_embeddings = chunk_embeddings / np.linalg.norm(chunk_embeddings)  # normalizes length to 1\n",
    "        chunk_embeddings = chunk_embeddings.tolist()\n",
    "    return chunk_embeddings\n",
    "\n",
    "def compute_doc_embeddings(df: pd.DataFrame) -> dict[tuple[str, str], list[float]]:\n",
    "    \"\"\"\n",
    "    Create an embedding for each row in the dataframe using the OpenAI Embeddings API.\n",
    "    \n",
    "    Return a dictionary that maps between each embedding vector and the index of the row that it corresponds to.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        idx: get_embedding(r.content) for idx, r in df.iterrows()\n",
    "    }\n",
    "\n",
    "def load_embeddings(folder_loc):\n",
    "    embedding_list = []\n",
    "    for item in glob(folder_loc):\n",
    "        with open(item,'rb') as f:\n",
    "            embedding_list.append({'file':item.replace(\"pickle\", 'txt'), 'embedding':pickle.loads(f.read())})\n",
    "    return embedding_list\n",
    "\n",
    "def read_docs(item):\n",
    "\n",
    "    with open(item) as f:\n",
    "        return f.read()\n",
    "    \n",
    "def vector_similarity(x: list[float], y: list[float]) -> float:\n",
    "    \"\"\"\n",
    "    Returns the similarity between two vectors.\n",
    "    \n",
    "    Because OpenAI Embeddings are normalized to length 1, the cosine similarity is the same as the dot product.\n",
    "    \"\"\"\n",
    "    return np.dot(np.array(x), np.array(y))\n",
    "\n",
    "def order_document_sections_by_query_similarity(query: str, contexts: dict[(str, str), np.array]) -> list[(float, (str, str))]:\n",
    "    \"\"\"\n",
    "    Find the query embedding for the supplied query, and compare it against all of the pre-calculated document embeddings\n",
    "    to find the most relevant sections. \n",
    "    \n",
    "    Return the list of document sections, sorted by relevance in descending order.\n",
    "    \"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    document_similarities = sorted([\n",
    "        (vector_similarity(query_embedding, rows['embedding']), read_docs(rows['file']), rows['file']) for rows in contexts\n",
    "], reverse=True)\n",
    "    \n",
    "    return document_similarities\n",
    "\n",
    "def construct_prompt(question: str, context_embeddings: dict) -> str:\n",
    "    \"\"\"\n",
    "    Fetch relevant \n",
    "    \"\"\"\n",
    "    most_relevant_document_sections = order_document_sections_by_query_similarity(question, context_embeddings)\n",
    "\n",
    "    \n",
    "    chosen_sections = []\n",
    "    chosen_sections_len = 0\n",
    "    chosen_sections_indexes = []\n",
    "     \n",
    "    chosoen_selections = most_relevant_document_sections[0]\n",
    "    # print(chosoen_selections)\n",
    "    # Useful diagnostic information\n",
    "    # print(f\"Selected {len(chosen_sections)} document sections:\")\n",
    "    # print(\"\\n\".join(chosen_sections_indexes))\n",
    "    \n",
    "    header = \"\"\"Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don't know.\"\\n\\nContext:\\n\"\"\"\n",
    "    \n",
    "    return header + \"\".join(chosoen_selections[1]) + \"\\n\\n Q: \" + question + \"\\n A:\", chosoen_selections[2]\n",
    "\n",
    "def answer_query_with_context(\n",
    "    query: str,\n",
    "    document_embeddings: dict[(str, str), np.array],\n",
    "    show_prompt: bool = True\n",
    ") -> str:\n",
    "    prompt, doc = construct_prompt(\n",
    "        query,\n",
    "        document_embeddings,\n",
    "    )\n",
    "    \n",
    "    if show_prompt:\n",
    "        print(prompt)\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "                prompt=prompt,\n",
    "                **COMPLETIONS_API_PARAMS\n",
    "            )\n",
    "\n",
    "    return response[\"choices\"][0][\"text\"].strip(\" \\n\"), doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7143547-7da3-4f23-a220-7f2c76a5f806",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file = \"230501_한 여행자휴대품통관 길라잡이_short.txt\"\n",
    "pickle_path = txt_file.replace(os.path.splitext(txt_file)[-1], \".pickle\")\n",
    "\n",
    "with open(txt_file, \"r\")as f: # read txt file\n",
    "    text = f.read()\n",
    "    \n",
    "with open(pickle_path, \"wb\")as f: # embedding & dump pickle\n",
    "    pickle.dump(len_safe_get_embedding(text, average=True), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33684ba2-477c-484b-b4b0-f77da85b08a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===\n",
      " 니트로글리세린은 면세범위가 없습니다.\n"
     ]
    }
   ],
   "source": [
    "embedding_data = load_embeddings(pickle_path) # load embeddings\n",
    "\n",
    "question = \"니트로글리세린은 면세범위가 어떻게 돼?\"\n",
    "\n",
    "prompt, doc = answer_query_with_context(question,  embedding_data, show_prompt = False)\n",
    "print(\"===\\n\", prompt)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seunghoon",
   "language": "python",
   "name": "seunghoon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
